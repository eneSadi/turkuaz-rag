{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"mlsum_train_dataset\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"mlsum_similar_news.pkl\", \"rb\") as f:\n",
    "    mlsum_similar_news = pickle.load(f)"
   ],
   "id": "d209979472ee3b97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"inference_question_NER_keyword_extraction_outputs.jsonl\", \"r\") as f:\n",
    "    ner_keyword_outputs = [json.loads(line) for line in f]"
   ],
   "id": "414bd938a6bf39ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_text_with_max_500_words(text):\n",
    "    words = text.split()\n",
    "    if len(words) > 500:\n",
    "        return \" \".join(words[:500])\n",
    "    return text"
   ],
   "id": "180e07979deeb993",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def parse_text_to_dictionary(text):\n",
    "    \"\"\"\n",
    "    Parses the input text into a structured dictionary format.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing sentences, keywords, and NER information.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with sentences, keywords, and NER information.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the text format does not match the expected pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {\"sentences\": []}\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"\\d+\\. Cümle: (?P<sentence>.*?)\\s*Keyword:\\s*(?P<keyword>.*?)\\s*NER:\\s*(?P<ner>.*?)(?=\\n\\d+\\. Cümle:|\\Z)\",\n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    matches = pattern.finditer(text)\n",
    "\n",
    "    if not matches:\n",
    "        raise ValueError(\"Input text does not match the expected format.\")\n",
    "\n",
    "    for match in matches:\n",
    "        sentence = match.group(\"sentence\").strip()\n",
    "        keyword = match.group(\"keyword\").strip()\n",
    "        ner = match.group(\"ner\").strip()\n",
    "        ner_list = [item.strip() for item in ner.split(\",\") if item]\n",
    "        # eliminate '-', '(boş)' phrases from the NER list\n",
    "        ner_list = [item for item in ner_list if item not in [\"-\", \"(boş)\", \"(yok)\", \"(Boş)\", \"(Yok)\", \"- (Yok)\"]]\n",
    "        data[\"sentences\"].append({\n",
    "            \"sentence\": sentence,\n",
    "            \"keyword\": keyword,\n",
    "            \"NER\": ner_list\n",
    "        })\n",
    "    if not data[\"sentences\"]:\n",
    "        raise ValueError(\"No valid sentences found in the input text.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "input_text = ner_keyword_outputs[0][\"answer\"]\n",
    "\n",
    "parsed_data = parse_text_to_dictionary(input_text)\n",
    "print(parsed_data)"
   ],
   "id": "2cba384a15aeaa3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for ner_keyword_output in ner_keyword_outputs:\n",
    "    input_text = ner_keyword_output[\"answer\"]\n",
    "    parsed_data = parse_text_to_dictionary(input_text)\n",
    "    ner_keyword_output[\"parsed_ner_keywords\"] = parsed_data\n",
    "    ner_keyword_output[\"ner_set\"] = set([ner for sentence in parsed_data[\"sentences\"] for ner in sentence[\"NER\"]])\n",
    "    ner_keyword_output[\"keyword_set\"] = set([keyword for sentence in parsed_data[\"sentences\"] for keyword in sentence[\"keyword\"].split(\",\")])"
   ],
   "id": "c1e64cd597eda091",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def match_news(news1, news2):\n",
    "    ner1 = news1[\"ner_set\"]\n",
    "    ner2 = news2[\"ner_set\"]\n",
    "    keyword1 = news1[\"keyword_set\"]\n",
    "    keyword2 = news2[\"keyword_set\"]\n",
    "\n",
    "    common_ners = ner1.intersection(ner2)\n",
    "    common_ner_count = len(common_ners)\n",
    "\n",
    "    common_keywords = keyword1.intersection(keyword2)\n",
    "    common_keyword_count = len(common_keywords)\n",
    "    return common_ner_count, common_keyword_count, common_ners, common_keywords\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "common_ner_keywords = []\n",
    "\n",
    "for i in tqdm(range(len(ner_keyword_outputs))):\n",
    "    for j in range(i + 1, len(ner_keyword_outputs)):\n",
    "        common_ner_count, common_keyword_count, common_ners, common_keywords = match_news(ner_keyword_outputs[i], ner_keyword_outputs[j])\n",
    "\n",
    "        news_1_date = dataset[ner_keyword_outputs[i][\"news_id\"]][\"date\"]\n",
    "        news_2_date = dataset[ner_keyword_outputs[j][\"news_id\"]][\"date\"]\n",
    "\n",
    "        if ((common_ner_count > 0 and common_keyword_count > 0) or (common_ner_count >= 2)) and (news_1_date != news_2_date):\n",
    "            common_ner_keywords.append({\n",
    "                \"news_1_id\": ner_keyword_outputs[i][\"news_id\"],\n",
    "                \"news_2_id\": ner_keyword_outputs[j][\"news_id\"],\n",
    "                \"common_ners\": common_ners,\n",
    "                \"common_keywords\": common_keywords\n",
    "            })"
   ],
   "id": "4bca4685b4fbeb9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(common_ner_keywords)"
   ],
   "id": "5cd3f88ab004cb96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "inference_question_templates = []\n",
    "counter = 0\n",
    "for common_item_duo in tqdm(common_ner_keywords, total=len(common_ner_keywords)):\n",
    "\n",
    "    first_news_id = common_item_duo[\"news_1_id\"]\n",
    "    second_news_id = common_item_duo[\"news_2_id\"]\n",
    "    common_ners = \", \".join(common_item_duo[\"common_ners\"])\n",
    "\n",
    "    # find the first news id in ner_keyword_outputs\n",
    "    first_news_idx = -1\n",
    "    for idx, ner_keyword_output in enumerate(ner_keyword_outputs):\n",
    "        if ner_keyword_output[\"news_id\"] == first_news_id:\n",
    "            first_news_idx = idx\n",
    "            break\n",
    "\n",
    "    if first_news_idx == -1:\n",
    "        print(f\"News with id {first_news_id} not found in ner_keyword_outputs\")\n",
    "        continue\n",
    "\n",
    "    # find the second news id in ner_keyword_outputs\n",
    "    second_news_idx = -1\n",
    "    for idx, ner_keyword_output in enumerate(ner_keyword_outputs):\n",
    "        if ner_keyword_output[\"news_id\"] == second_news_id:\n",
    "            second_news_idx = idx\n",
    "            break\n",
    "\n",
    "    if second_news_idx == -1:\n",
    "        print(f\"News with id {second_news_id} not found in ner_keyword_outputs\")\n",
    "        continue\n",
    "\n",
    "    summary_sentences_news_1 = \"\\n\".join([sentence[\"sentence\"] for sentence in ner_keyword_outputs[first_news_idx][\"parsed_ner_keywords\"][\"sentences\"]])\n",
    "    summary_sentences_news_2 = \"\\n\".join([sentence[\"sentence\"] for sentence in ner_keyword_outputs[second_news_idx][\"parsed_ner_keywords\"][\"sentences\"]])\n",
    "\n",
    "    news = dataset[first_news_id]\n",
    "    similar_sample = dataset[second_news_id]\n",
    "\n",
    "    news_text = get_text_with_max_500_words(news['text'])\n",
    "    similar_sample_text = get_text_with_max_500_words(similar_sample['text'])\n",
    "\n",
    "    inference_question_template = f\"\"\"A multi-hop question is a question that is requiring multiple inferential leaps or accessing several pieces of information from different locations or sources to arrive at an answer. The following are the metadata of 2 news articles, summaries of articles, 5 summarizing sentences for each news and texts of the news. Both news are related to the same topic and have the same entity (or entities): {common_ners}. Your task is to generate one multi-hop inference question based on the news. Here are some instructions:\n",
    "1. Find the Connection: The connection between news is entity (or entities): {common_ners}, which is how this key piece of information is related or how they can be combined to form a more complex idea.\n",
    "2. Formulate the Question: Create a question that cannot be answered by relying on just one of the sentences but instead requires understanding and linking the information from all of the sources. The answer must be the entity (or one of the entities): {common_ners}.\n",
    "3. Ensure Coherence: Make sure the question flows logically from the combined information and is clear and unambiguous.\n",
    "4. Do not use the answer in the question: You must use other details from both news in the question but do not use the entity (answer) in the question.\n",
    "5. Generate the answer: Generate the answer for the question, it must be the entity (or one of the entities): {common_ners}.\n",
    "(the question and the answer should be in Turkish)\n",
    "\n",
    "News 1:\n",
    "Title: {news['title']}\n",
    "Date: {news['date']}\n",
    "Summary: {news['summary']}\n",
    "Text: {news['text']}\n",
    "Summary Sentences: {summary_sentences_news_1}\n",
    "\n",
    "News 2:\n",
    "Title: {similar_sample['title']}\n",
    "Date: {similar_sample['date']}\n",
    "Summary: {similar_sample['summary']}\n",
    "Text: {similar_sample['text']}\n",
    "Summary Sentences: {summary_sentences_news_2}\n",
    "\n",
    "Your answer should be in the following format:\n",
    "Question:\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    inference_question_templates.append({\"prompt\": inference_question_template, \"news_1_id\": first_news_id, \"news_2_id\": second_news_id})\n",
    "    counter += 1"
   ],
   "id": "b0b58233bd3c56a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"inference_question_templates.json\", \"w\") as f:\n",
    "    json.dump(inference_question_templates, f)"
   ],
   "id": "93d50dd22e971382",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "790bef80d6bc2804",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
