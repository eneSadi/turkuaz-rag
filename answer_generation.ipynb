{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"mlsum_train_dataset\")\n",
    "\n",
    "dataset"
   ],
   "id": "5b3ab48058b85235",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"comparison_question_templates_outputs.jsonl\", \"r\") as f:\n",
    "    comparison_openai_outputs = [json.loads(line) for line in f]\n",
    "\n",
    "print(comparison_openai_outputs[0].keys())\n",
    "comparison_openai_outputs[0][\"answer\"]"
   ],
   "id": "2070934a24f95a57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"temporal_question_templates_outputs.jsonl\", \"r\") as f:\n",
    "    temporal_openai_outputs = [json.loads(line) for line in f]\n",
    "\n",
    "print(temporal_openai_outputs[0].keys())\n",
    "temporal_openai_outputs[0][\"answer\"]"
   ],
   "id": "374b1d3b79c0d6af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"inference_question_templates_outputs.jsonl\", \"r\") as f:\n",
    "    inference_openai_outputs = [json.loads(line) for line in f]\n",
    "\n",
    "inference_openai_outputs[0].keys()\n",
    "inference_openai_outputs[0][\"answer\"]"
   ],
   "id": "2e1e1f4ad89d5bd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"inference_question_templates_outputs_v2.jsonl\", \"r\") as f:\n",
    "    inference_openai_outputs_v2  = [json.loads(line) for line in f]\n",
    "\n",
    "inference_openai_outputs_v2[0].keys()\n",
    "inference_openai_outputs_v2[0][\"answer\"]"
   ],
   "id": "f586fa2bbdc1a195",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"context_fusion_question_templates_outputs.jsonl\", \"r\") as f:\n",
    "    contex_fusion_openai_outputs  = [json.loads(line) for line in f]\n",
    "\n",
    "contex_fusion_openai_outputs[0].keys()\n",
    "contex_fusion_openai_outputs[0][\"answer\"]"
   ],
   "id": "aeb2b26aa01cab35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"null_question_templates_outputs.jsonl\", \"r\") as f:\n",
    "    null_openai_outputs  = [json.loads(line) for line in f]\n",
    "\n",
    "null_openai_outputs[0].keys()\n",
    "null_openai_outputs[0][\"answer\"]"
   ],
   "id": "a540bbc549867f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def extract_qa(text):\n",
    "    question_patterns = [r\"(?:Question|Soru):\\s*(.+?)(?=(?:\\n|Answer|Cevap|$))\"]\n",
    "    answer_patterns = [r\"(?:Answer|Cevap):\\s*(.+?)(?=\\n|$)\"]\n",
    "\n",
    "    question = None\n",
    "    answer = None\n",
    "\n",
    "    answer_match = None\n",
    "    for pattern in answer_patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            answer_match = match\n",
    "            answer = match.group(1).strip()\n",
    "            break\n",
    "\n",
    "    for pattern in question_patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            question = match.group(1).strip()\n",
    "            break\n",
    "\n",
    "    if not question and answer_match:\n",
    "        question = text[:answer_match.start()].strip()\n",
    "\n",
    "    if not question:\n",
    "        sentences = re.split(r\"\\n+\", text.strip())\n",
    "        for sentence in sentences:\n",
    "            if '?' in sentence:\n",
    "                question = sentence.strip()\n",
    "                break\n",
    "\n",
    "    if not question:\n",
    "        print(\"Error: No question found.\")\n",
    "        return None\n",
    "\n",
    "    return {\"question\": question, \"answer\": answer}"
   ],
   "id": "17f7bfc72a0c515d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_openai_outputs = [inference_openai_outputs, inference_openai_outputs_v2, comparison_openai_outputs, temporal_openai_outputs, contex_fusion_openai_outputs, null_openai_outputs]",
   "id": "52d20c0993f01ab9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_simple_prompt_for_answer(question, question_type, news_articles):\n",
    "    \"\"\"\n",
    "    Generate a prompt for the answer generation model based on the question type and news articles.\n",
    "    \"\"\"\n",
    "    prompt = \"You are given two Turkish news articles. Use the information in both articles to answer the question below.\\n\\n\"\n",
    "\n",
    "    for i, article in enumerate(news_articles):\n",
    "        prompt += f\"News Article {i + 1}:\\n\"\n",
    "        prompt += f\"Title: {article['title']}\\n\"\n",
    "        prompt += f\"Date: {article['date']}\\n\"\n",
    "        prompt += f\"Summary: {article['summary']}\\n\"\n",
    "        prompt += f\"Text: {article['text']}\\n\\n\"\n",
    "\n",
    "    prompt += f\"Question Type: {question_type}\\n\"\n",
    "\n",
    "    # Shared warnings\n",
    "    general_rules = (\n",
    "        \"⚠️ Please follow these answer rules:\\n\"\n",
    "        \"1. Keep your answer as **short and concise** as possible. **Do not provide explanations** or justifications.\\n\"\n",
    "        \"2. If **any part of the question lacks sufficient information** in the provided article(s), you must respond with **'Yetersiz bilgi'**.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    if question_type == \"Inference Question\":\n",
    "        prompt += (\n",
    "            \"This is an inference question. You must answer it by combining information from both articles. \"\n",
    "            \"The answer must be one of the shared entities mentioned across both texts if the provided article(s) are enough to answer. \"\n",
    "            \"Do not hallucinate. If the context is not sufficient to answer confidently, output only 'Yetersiz bilgi'.\\n\\n\"\n",
    "        )\n",
    "    elif question_type == \"Comparison Question\":\n",
    "        prompt += (\n",
    "            \"This is a comparison question. Compare factual information (quantities, opinions, descriptions) \"\n",
    "            \"from both articles to generate an answer. Use comparative expressions like 'daha fazla', 'aynı', or 'hayır' if appropriate. \"\n",
    "            \"If the articles do not provide enough information for a valid comparison, answer with 'Yetersiz bilgi'.\\n\\n\"\n",
    "        )\n",
    "    elif question_type == \"Temporal Question\":\n",
    "        prompt += (\n",
    "            \"This is a temporal question. Answer using time-based reasoning from both articles, such as which came first, duration, or change over time. \"\n",
    "            \"Do not use explicit dates. Focus on the chronological relationships or patterns described. \"\n",
    "            \"If the time-based relationship is unclear or missing, respond with 'Yetersiz bilgi'.\\n\\n\"\n",
    "        )\n",
    "    elif question_type == \"Context Fusion Question\":\n",
    "        prompt += (\n",
    "            \"This is a context fusion question. You must answer it by combining information from both articles. \"\n",
    "            \"If a proper answer is not possible due to lack of information, respond with 'Yetersiz bilgi'.\\n\\n\"\n",
    "        )\n",
    "    elif question_type == \"Null Question\":\n",
    "        prompt += (\n",
    "            \"This is a null question. While it may sound valid, it cannot be answered using the information in the articles. \"\n",
    "            \"Carefully review the texts and if you find that the question cannot be answered with certainty, output only 'Yetersiz bilgi'. \"\n",
    "            \"Do not attempt to guess or hallucinate.\\n\\n\"\n",
    "        )\n",
    "    else:\n",
    "        prompt += \"This question type is unknown. Use your best judgment but do not hallucinate.\\n\\n\"\n",
    "\n",
    "    prompt += general_rules\n",
    "    prompt += f\"Soru: {question}\\n\"\n",
    "    prompt += \"Cevap: \"\n",
    "    return prompt\n"
   ],
   "id": "de0f9ccd62bb3f9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_prompt_for_final_answer(question, question_type, news_articles):\n",
    "    \"\"\"\n",
    "    Generate a prompt to get a final answer from the LLM for a given question and two news articles.\n",
    "    'Yetersiz bilgi' (Insufficient info) is NOT allowed unless the question type is Null Question.\n",
    "    \"\"\"\n",
    "    assert len(news_articles) == 2, \"Exactly two news articles must be provided.\"\n",
    "\n",
    "    prompt = \"You are given two Turkish news articles. Read them carefully and answer the question below.\\n\\n\"\n",
    "\n",
    "    for i, article in enumerate(news_articles):\n",
    "        prompt += f\"News Article {i + 1}:\\n\"\n",
    "        prompt += f\"Title: {article['title']}\\n\"\n",
    "        prompt += f\"Date: {article['date']}\\n\"\n",
    "        prompt += f\"Summary: {article['summary']}\\n\"\n",
    "        prompt += f\"Text: {article['text']}\\n\\n\"\n",
    "\n",
    "    prompt += f\"Question Type: {question_type}\\n\\n\"\n",
    "\n",
    "    # Shared instructions\n",
    "    prompt += (\n",
    "        \"⚠️ Please follow these answer guidelines:\\n\"\n",
    "        \"1. Your answer should be **as short and concise as possible**. **Do not explain or justify your answer.**\\n\"\n",
    "    )\n",
    "\n",
    "    if question_type == \"Null Question\":\n",
    "        prompt += (\n",
    "            \"2. If the answer cannot be found in the articles, respond only with **'Yetersiz bilgi'** (meaning 'Insufficient information').\\n\\n\"\n",
    "        )\n",
    "    else:\n",
    "        prompt += (\n",
    "            \"2. The answer **can be found in the articles**. Do **not** respond with 'Yetersiz bilgi'. \"\n",
    "            \"Answer the question directly based on the content.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    # Type-specific instructions\n",
    "    if question_type == \"Inference Question\":\n",
    "        prompt += (\n",
    "            \"This is an inference question. The answer should be an entity (such as a person, organization, country) \"\n",
    "            \"that is mentioned in both articles and can be inferred from combining their content.\\n\\n\"\n",
    "        )\n",
    "    elif question_type == \"Comparison Question\":\n",
    "        prompt += (\n",
    "            \"This is a comparison question. Use factual comparisons (quantities, opinions, descriptions) from both articles \"\n",
    "            \"and answer using expressions like 'daha fazla' (more), 'aynı' (same), or 'hayır' (no) where appropriate.\\n\\n\"\n",
    "        )\n",
    "    elif question_type == \"Temporal Question\":\n",
    "        prompt += (\n",
    "            \"This is a temporal question. Use time-based reasoning from the articles, such as what happened first, \"\n",
    "            \"lasted longer, or how something evolved over time.\\n\\n\"\n",
    "        )\n",
    "    elif question_type == \"Context Fusion Question\":\n",
    "        prompt += (\n",
    "            \"This is a context fusion question. You must answer it by combining information from both articles.\\n\\n\"\n",
    "        )\n",
    "    elif question_type == \"Null Question\":\n",
    "        prompt += (\n",
    "            \"This is a null question. It may appear answerable, but it lacks sufficient information in the articles. \"\n",
    "            \"If the answer cannot be confidently derived, reply only with **'Yetersiz bilgi'**.\\n\\n\"\n",
    "        )\n",
    "    else:\n",
    "        prompt += (\n",
    "            \"This is a generic question. Provide a direct and concise answer using the available content. Do not speculate.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += f\"Question: {question}\\n\"\n",
    "    prompt += \"Answer: \"\n",
    "    return prompt\n"
   ],
   "id": "1fd443fbfc468a75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "hf_records = []\n",
    "\n",
    "i = 0\n",
    "for openai_outputs in all_openai_outputs:\n",
    "    i += 1\n",
    "    for output in openai_outputs:\n",
    "        required_keys = [\"answer\", \"prompt\", \"news_1_id\", \"news_2_id\"]\n",
    "        missing_keys = [key for key in required_keys if key not in output]\n",
    "        if missing_keys:\n",
    "            print(f\"Error: Missing keys {missing_keys} in output.\")\n",
    "            continue\n",
    "\n",
    "        question_answer = extract_qa(output[\"answer\"])\n",
    "        if question_answer is None:\n",
    "            print(\"Error: No valid question found in QA extraction.\")\n",
    "            print(f\"Text: {output['answer']}\")\n",
    "            continue\n",
    "\n",
    "        news_1_id = output[\"news_1_id\"]\n",
    "        news_2_id = output[\"news_2_id\"]\n",
    "\n",
    "        news_1 = dataset[news_1_id]\n",
    "        news_2 = dataset[news_2_id]\n",
    "\n",
    "        question_type = \"\"\n",
    "        if i == 1:\n",
    "            question_type = \"Inference Question\"\n",
    "        elif i == 2:\n",
    "            question_type = \"Inference Question\"\n",
    "        elif i == 3:\n",
    "            question_type = \"Comparison Question\"\n",
    "        elif i == 4:\n",
    "            question_type = \"Temporal Question\"\n",
    "        elif i == 5:\n",
    "            question_type = \"Context Fusion Question\"\n",
    "        elif i == 6:\n",
    "            question_type = \"Null Question\"\n",
    "        else:\n",
    "            print(\"Error: Invalid question type.\")\n",
    "            continue\n",
    "\n",
    "        record = {\n",
    "            \"question_type\": question_type,\n",
    "            \"1st_news_id\": news_1_id,\n",
    "            \"2nd_news_id\": news_2_id,\n",
    "            \"1st_news\": {\n",
    "                \"title\": news_1['title'],\n",
    "                \"date\": news_1['date'],\n",
    "                \"summary\": news_1['summary'],\n",
    "                \"text\": news_1['text']\n",
    "            },\n",
    "            \"2nd_news\": {\n",
    "                \"title\": news_2['title'],\n",
    "                \"date\": news_2['date'],\n",
    "                \"summary\": news_2['summary'],\n",
    "                \"text\": news_2['text']\n",
    "            },\n",
    "            \"question\": question_answer[\"question\"],\n",
    "            \"answer\": None\n",
    "        }\n",
    "\n",
    "        answer = question_answer.get(\"answer\")\n",
    "        if answer and answer.strip() != \"\":\n",
    "            record[\"answer\"] = answer\n",
    "        else:\n",
    "            if i == 1 or i == 2:\n",
    "                print(\"Warning: No valid answer found.\")\n",
    "\n",
    "        # randomly select one of the news articles\n",
    "        single_news = random.choice([news_1, news_2])\n",
    "        # randomly select one of the news articles from the whole dataset\n",
    "        random_news = dataset[random.randint(0, len(dataset) - 1)]\n",
    "\n",
    "        # generate a random probability between 0 and 1\n",
    "        random_probability = random.random()\n",
    "        # if the random probability is less than 0.5, use the first news article\n",
    "        if random_probability < 0.5:\n",
    "            record[\"prompt_with_single_news\"] = generate_simple_prompt_for_answer(question_answer[\"question\"], question_type,  [single_news, random_news])\n",
    "        else:\n",
    "            record[\"prompt_with_single_news\"] = generate_simple_prompt_for_answer(question_answer[\"question\"], question_type, [random_news, single_news])\n",
    "\n",
    "        record[\"prompt_with_both_news\"] = generate_simple_prompt_for_answer(question_answer[\"question\"], question_type, [news_1, news_2])\n",
    "        record[\"prompt_for_final_answer\"] = generate_prompt_for_final_answer(question_answer[\"question\"], question_type, [news_1, news_2])\n",
    "\n",
    "        record[\"answer_with_single_news\"] = None\n",
    "        record[\"answer_with_both_news\"] = None\n",
    "        record[\"final_answer_for_dataset\"] = None\n",
    "\n",
    "        hf_records.append(record)"
   ],
   "id": "27ca00a6e2a0aaab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"huggingface_dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for record in hf_records:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ],
   "id": "f9a4c14c9f804875",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Answer Validation",
   "id": "851a7603aba01f97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"huggingface_dataset_with_answers.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hf_records_with_answers = [json.loads(line) for line in f]\n",
    "\n",
    "# Check the first record\n",
    "print(hf_records_with_answers[0].keys())\n",
    "print(len(hf_records_with_answers))"
   ],
   "id": "6637e7272e48bb95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# find unique question types\n",
    "question_types = set()\n",
    "for record in hf_records_with_answers:\n",
    "    question_types.add(record[\"question_type\"])\n",
    "\n",
    "print(question_types)"
   ],
   "id": "99d9f407aee6a2e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# select random 5 records for each question type\n",
    "import random\n",
    "\n",
    "question_type_records = {qt: [] for qt in question_types}\n",
    "for record in hf_records_with_answers:\n",
    "    question_type_records[record[\"question_type\"]].append(record)\n",
    "\n",
    "random_records = {}\n",
    "for qt, records in question_type_records.items():\n",
    "    random_records[qt] = random.sample(records, 5)\n",
    "\n",
    "# Print the random records\n",
    "for qt, records in random_records.items():\n",
    "    print(f\"Question Type: {qt}\")\n",
    "    for record in records:\n",
    "        print(f\"Record ID: {record['1st_news_id']}, {record['2nd_news_id']}\")\n",
    "        print(f\"Question: {record['question']}\")\n",
    "        print(f\"Given Answer: {record['answer']}\")\n",
    "        print(f\"Generated with Single News Answer: {record['answer_with_single_news']}\")\n",
    "        print(f\"Generated with Both News Answer: {record['answer_with_both_news']}\")\n",
    "        print(\"-\" * 50)"
   ],
   "id": "c123a64fcf0ca60a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# transform hf_records_with_answers to a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "hf_records_df = pd.DataFrame(hf_records_with_answers)\n",
    "hf_records_df"
   ],
   "id": "dc8629a3f64d6352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# find the number of rows that includes \"Yetersiz bilgi\" in answer_with_single_news column\n",
    "yetersiz_bilgi_count_both_news = hf_records_df[hf_records_df[\"answer_with_both_news\"].str.contains(\"Yetersiz bilgi\", na=False)].shape[0]\n",
    "print(yetersiz_bilgi_count_both_news)"
   ],
   "id": "d346d1fffb835b9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yetersiz_bilgi_count_single_news = hf_records_df[hf_records_df[\"answer_with_single_news\"].str.contains(\"Yetersiz bilgi\", na=False)].shape[0]\n",
    "print(yetersiz_bilgi_count_single_news)"
   ],
   "id": "5f6979d179a59892",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# find the number of rows that includes \"Yetersiz bilgi\" in answer_with_both_news column by question type\n",
    "yetersiz_bilgi_count_both_news_by_question_type = hf_records_df[hf_records_df[\"answer_with_both_news\"].str.contains(\"Yetersiz bilgi\", na=False)].groupby(\"question_type\").size()\n",
    "\n",
    "yetersiz_bilgi_count_both_news_by_question_type = yetersiz_bilgi_count_both_news_by_question_type.reset_index(name=\"count\")\n",
    "yetersiz_bilgi_count_both_news_by_question_type"
   ],
   "id": "98bde946463bab6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yetersiz_bilgi_count_single_news_by_question_type = hf_records_df[hf_records_df[\"answer_with_single_news\"].str.contains(\"Yetersiz bilgi\", na=False)].groupby(\"question_type\").size()\n",
    "\n",
    "yetersiz_bilgi_count_single_news_by_question_type = yetersiz_bilgi_count_single_news_by_question_type.reset_index(name=\"count\")\n",
    "yetersiz_bilgi_count_single_news_by_question_type"
   ],
   "id": "150b116aad1d3298",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yetersiz_bilgi_count_single_news_by_question_type = hf_records_df[hf_records_df[\"final_answer_for_dataset\"].str.contains(\"Yetersiz bilgi\", na=False)].groupby(\"question_type\").size()\n",
    "\n",
    "yetersiz_bilgi_count_single_news_by_question_type = yetersiz_bilgi_count_single_news_by_question_type.reset_index(name=\"count\")\n",
    "yetersiz_bilgi_count_single_news_by_question_type"
   ],
   "id": "17811184dd3993b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "68bf2f781f9f5a7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
